{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d77e24d6",
   "metadata": {},
   "source": [
    "### Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d5de09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No text found for item 16158\n",
      "Warning: No text found for item 16159\n",
      "Conversion complete. Created 1757 train, 376 validation, and 378 test samples.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "def convert_label_studio_to_pyabsa(input_file, output_dir):\n",
    "    \"\"\"\n",
    "    Convert Label Studio export format to PyABSA EMCGCN format.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Read the Label Studio export file\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    converted_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        # Skip if no annotations\n",
    "        if not item.get('annotations'):\n",
    "            continue\n",
    "        \n",
    "        # Get the text from the item\n",
    "        text = item.get('data', {}).get('text', '')\n",
    "        \n",
    "        # If text is not found in the expected location, try to find it elsewhere\n",
    "        if not text and 'text' in item:\n",
    "            text = item['text']\n",
    "        \n",
    "        # Skip if no text\n",
    "        if not text:\n",
    "            print(f\"Warning: No text found for item {item['id']}\")\n",
    "            continue\n",
    "        \n",
    "        for annotation in item['annotations']:\n",
    "            aspects = []\n",
    "            opinions = []\n",
    "            relations = []\n",
    "            \n",
    "            # Extract aspects, opinions, and relations from the annotation results\n",
    "            for result in annotation['result']:\n",
    "                if result['type'] == 'labels':\n",
    "                    # Get the value data\n",
    "                    value = result.get('value', {})\n",
    "                    \n",
    "                    # If there's text and labels\n",
    "                    if value and 'text' in value and 'labels' in value:\n",
    "                        # Calculate word indices\n",
    "                        start_char = value['start']\n",
    "                        end_char = value['end']\n",
    "                        \n",
    "                        # Extract words from the text and determine indices\n",
    "                        words = text.split()\n",
    "                        char_index = 0\n",
    "                        word_indices = []\n",
    "                        \n",
    "                        for i, word in enumerate(words):\n",
    "                            word_len = len(word)\n",
    "                            # Check if this word overlaps with our span\n",
    "                            if not (char_index + word_len <= start_char or char_index >= end_char):\n",
    "                                word_indices.append(i)\n",
    "                            char_index += word_len + 1  # +1 for the space\n",
    "                        \n",
    "                        if 'aspect' in value['labels']:\n",
    "                            aspects.append({\n",
    "                                'id': result['id'],\n",
    "                                'indices': word_indices,\n",
    "                                'text': value['text']\n",
    "                            })\n",
    "                        elif 'opinion' in value['labels']:\n",
    "                            opinions.append({\n",
    "                                'id': result['id'],\n",
    "                                'indices': word_indices,\n",
    "                                'text': value['text']\n",
    "                            })\n",
    "                \n",
    "                elif result['type'] == 'relation':\n",
    "                    from_id = result['from_id']\n",
    "                    to_id = result['to_id']\n",
    "                    labels = result.get('labels', [])\n",
    "                    sentiment = labels[0] if labels else 'NEU'\n",
    "                    relations.append({\n",
    "                    'from_id': from_id,\n",
    "                    'to_id': to_id,\n",
    "                    'sentiment': sentiment\n",
    "                })\n",
    "\n",
    "            \n",
    "            # Generate PyABSA format triplets\n",
    "            triplets = []\n",
    "            for relation in relations:\n",
    "                aspect = next((a for a in aspects if a['id'] == relation['from_id']), None)\n",
    "                opinion = next((o for o in opinions if o['id'] == relation['to_id']), None)\n",
    "                \n",
    "                if aspect and opinion:\n",
    "                    triplets.append((aspect['indices'], opinion['indices'], relation['sentiment']))\n",
    "            \n",
    "            if triplets:\n",
    "                # Create the PyABSA format string\n",
    "                pyabsa_format = f\"{text}####[{', '.join(str(t) for t in triplets)}]\"\n",
    "                converted_data.append(pyabsa_format)\n",
    "    \n",
    "    # Shuffle data for randomness\n",
    "    random.shuffle(converted_data)\n",
    "    \n",
    "    # Split data into train, validation, and test sets (70/15/15 split)\n",
    "    n = len(converted_data)\n",
    "    train_size = int(0.7 * n)\n",
    "    val_size = int(0.15 * n)\n",
    "    \n",
    "    train_data = converted_data[:train_size]\n",
    "    valid_data = converted_data[train_size:train_size + val_size]\n",
    "    test_data = converted_data[train_size + val_size:]\n",
    "    \n",
    "    # Write to output files\n",
    "    with open(os.path.join(output_dir, 'train.txt'), 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(train_data))\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'valid.txt'), 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(valid_data))\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'test.txt'), 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(test_data))\n",
    "    \n",
    "    print(f\"Conversion complete. Created {len(train_data)} train, {len(valid_data)} validation, and {len(test_data)} test samples.\")\n",
    "\n",
    "# Define input and output paths\n",
    "input_file = r\"C:\\Users\\gungi\\OneDrive\\Desktop\\Tugas Akhir\\Code\\Coba ABSA\\2. Coba ABSA\\data12_2500_NEWEST\\absa_2500data_newest.json\"\n",
    "output_dir = r\"C:\\Users\\gungi\\OneDrive\\Desktop\\Tugas Akhir\\Code\\Coba ABSA\\2. Coba ABSA\\data12_2500_NEWEST\"\n",
    "\n",
    "# Execute conversion\n",
    "convert_label_studio_to_pyabsa(input_file, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
